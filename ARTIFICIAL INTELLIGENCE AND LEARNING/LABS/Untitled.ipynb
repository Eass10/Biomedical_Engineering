{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4fe6e9",
   "metadata": {},
   "source": [
    "# Project 3: Parkinson's Dataset\n",
    "\n",
    "Project 2\n",
    "- Enrique Almazán Sánchez\n",
    "- Judith Briz Galera\n",
    "\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7602bd",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dda4510",
   "metadata": {},
   "source": [
    "## Previous concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632d8cd1",
   "metadata": {},
   "source": [
    "## Dependencies (Required Libraries)\n",
    "\n",
    "In the following cell, we import all the necessary libraries for the project.\n",
    "\n",
    "- **Pandas**: A Python library used for data analysis and manipulation. It provides flexible and efficient data structures, such as DataFrames, for working with tabular datasets. Pandas offers a wide range of functions and methods for cleaning, transforming, and exploring data, making the data preparation process easier before applying machine learning algorithms.\n",
    "\n",
    "- **NumPy**: A fundamental library for scientific computing in Python. It provides a data structure called a multidimensional array (ndarray) that allows for efficient operations on data arrays. NumPy is widely used in numerical analysis and data processing, providing functionality for mathematical operations, array manipulation, and statistical calculations.\n",
    "\n",
    "- **scipy.stats**: Python library within SciPy that focuses on statistical functions and probability distributions. It offers tools for working with probability distributions, statistical tests, random variables, descriptive statistics, and modeling. It's a versatile library used for statistical analysis and hypothesis testing in scientific research and data analysis.\n",
    "\n",
    "- **Matplotlib**: A data visualization library in Python. It provides a wide range of functions and methods for creating static plots, such as line plots, bar charts, scatter plots, and contour plots. Matplotlib is highly customizable and allows for adding labels, titles, legends, and other annotations to plots. It is a popular tool for data visualization in data analysis and result presentation.\n",
    "\n",
    "- **Plotly**: An interactive data visualization library for Python. It enables the creation of interactive and dynamic charts, including scatter plots, line charts, bar charts, and surface plots. Plotly offers a web-based user interface for exploring and manipulating charts, making it easy to create interactive visualizations and present data.\n",
    "\n",
    "- **Seaborn**: A data visualization library based on Matplotlib. It provides a high-level interface for creating attractive and concise statistical plots. Seaborn simplifies the creation of distribution plots, regression plots, correlation plots, and other common chart types in data analysis. Additionally, Seaborn offers predefined styles and color palettes that enhance the appearance of charts.\n",
    "\n",
    "- **Scikit-learn (sklearn)**: An open-source machine learning library for Python. It provides a wide range of algorithms and tools for performing machine learning tasks, such as classification, regression, clustering, and feature selection. Scikit-learn stands out for its ease of use and focus on efficiency and scalability. In addition to algorithms, the library also offers utilities for model evaluation, cross-validation, and data preprocessing.\n",
    "\n",
    "### 1.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b22206e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Feature Selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Figures of merit\n",
    "# metrics for linear regression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "# metrics for logistic regression\n",
    "from sklearn.metrics import accuracy_score, recall_score, roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "\n",
    "# Models to be implemented\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Lasso, Ridge, ElasticNet\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef0a718",
   "metadata": {},
   "source": [
    "### 1.2. Dataset import\n",
    "\n",
    "First of all, we import the clean dataset we obtained in Project 1, which was already split in train and test, but without normalizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60820e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_data_bcl.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c03b763970c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import train set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_data_bcl.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# Import test set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_data_bcl.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_data_bcl.csv'"
     ]
    }
   ],
   "source": [
    "# Import train set\n",
    "train = pd.read_csv('train_data_bcl.csv')\n",
    "# Import test set\n",
    "test = pd.read_csv('test_data_bcl.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459a88eb",
   "metadata": {},
   "source": [
    "Now divide each of the sets in features and target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4f6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide train set in features and target variable\n",
    "X_train, y_train = (train.drop('Fraction of unvoiced frames', axis=1), train['Fraction of unvoiced frames'])\n",
    "# Divide test set in features and target variable\n",
    "X_test, y_test = (test.drop('Fraction of unvoiced frames', axis=1), test['Fraction of unvoiced frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbb15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Finally, we can show each of the sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3d13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show features for training set\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show features for test set\n",
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e31f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show target variable for train set\n",
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8ab946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show target variable for test set\n",
    "y_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447b1860",
   "metadata": {},
   "source": [
    "### 1.3. Normalization\n",
    "\n",
    "A function is used in order to have different types of normalization and test which of them gives the better results for the posterior training, validation and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fb3afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing(tp, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Normalizes the input datasets using the specified scaler type.\n",
    "\n",
    "    Parameters:\n",
    "    - tp (str): Type of scaler to use. Options: \"ss\" for StandardScaler, \"mm\" for MinMaxScaler, \"rs\" for RobustScaler.\n",
    "    - X_train (array-like): Training feature dataset.\n",
    "    - X_test (array-like): Testing feature dataset.\n",
    "    - y_train (array-like): Training target variable.\n",
    "    - y_test (array-like): Testing target variable.\n",
    "\n",
    "    Returns:\n",
    "    Tuple of normalized feature datasets and target variables: (X_train_norm, y_train, X_test_norm, y_test).\n",
    "    \"\"\"\n",
    "\n",
    "    if tp == \"ss\":\n",
    "        # Dataset Normalization using StandardScaler for 'Status'\n",
    "        scaler = StandardScaler()\n",
    "    elif tp == \"mm\":\n",
    "        # Dataset Normalization using MinMaxScaler for 'Status'\n",
    "        scaler = MinMaxScaler()\n",
    "    elif tp == \"rs\":\n",
    "        # Dataset Normalization using RobustScaler for 'Status'\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "    # Fitting the scaler with the X_train subset and normalizing it\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    # Normalizing the X_test subset with respect to the values taken from X_train, as the scaler was trained with it\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    # Shuffle the normalized sets\n",
    "    X_train_norm, y_train = shuffle(X_train_norm, y_train, random_state=20)\n",
    "    X_test_norm, y_test = shuffle(X_test_norm, y_test, random_state=20)\n",
    "\n",
    "    return X_train_norm, y_train, X_test_norm, y_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
