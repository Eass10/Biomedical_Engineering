{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5e2df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Visualizing\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import (\n",
    "    MinMaxScaler, \n",
    "    StandardScaler, \n",
    "    RobustScaler\n",
    ")\n",
    "\n",
    "# Figures of merit\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    auc, \n",
    "    roc_auc_score, \n",
    "    roc_curve, \n",
    "    confusion_matrix, \n",
    "    classification_report, \n",
    "    r2_score, \n",
    "    mean_absolute_error, \n",
    "    mean_squared_error\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    StratifiedKFold,\n",
    "    RepeatedKFold,\n",
    "    KFold\n",
    ")\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ac9e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizing(tp, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Normalizes the input datasets using the specified scaler type.\n",
    "\n",
    "    Parameters:\n",
    "    - tp (str): Type of normalization to apply.\n",
    "    - X_train (DataFrame): Training feature dataset.\n",
    "    - X_test (DataFrame): Testing feature dataset.\n",
    "    - y_train (Series): Training target variable.\n",
    "    - y_test (Series): Testing target variable.\n",
    "\n",
    "    Outputs:\n",
    "    - X_train (DataFrame): Normalized training feature dataset.\n",
    "    - y_train (Series): Training target variable.\n",
    "    - X_test (DataFrame): Normalized testing feature dataset.\n",
    "    - y_test (Series): Testing target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    if tp == \"ss\":\n",
    "        # Dataset Normalization using StandardScaler for 'Status'\n",
    "        scaler = StandardScaler()\n",
    "    elif tp == \"mm\":\n",
    "        # Dataset Normalization using MinMaxScaler for 'Status'\n",
    "        scaler = MinMaxScaler()\n",
    "    elif tp == \"rs\":\n",
    "        # Dataset Normalization using RobustScaler for 'Status'\n",
    "        scaler = RobustScaler()\n",
    "\n",
    "    # Fitting the scaler with the X_train subset and normalizing it\n",
    "    X_train_norm = scaler.fit_transform(X_train)\n",
    "    # Normalizing the X_test subset with respect to the values taken from X_train, as the scaler was trained with it\n",
    "    X_test_norm = scaler.transform(X_test)\n",
    "    # Shuffle the normalized sets\n",
    "    X_train_norm, y_train = shuffle(X_train_norm, y_train, random_state=20)\n",
    "    X_test_norm, y_test = shuffle(X_test_norm, y_test, random_state=20)\n",
    "\n",
    "    return X_train_norm, y_train, X_test_norm, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50129c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def get_metrics(confmat):\n",
    "    '''\n",
    "    Unravel confusion matrix and calculate performance metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - confmat (array-like): Confusion matrix in the form of a 2D array or list.\n",
    "\n",
    "    Outputs:\n",
    "    - acc (float): Accuracy.\n",
    "    - sen (float): Sensitivity (True Positive Rate).\n",
    "    - esp (float): Specificity.\n",
    "    - ppv (float): Positive Predictive Value (Precision).\n",
    "    - fsc (float): F1 Score.\n",
    "    '''\n",
    "\n",
    "    # Unravel the confusion matrix\n",
    "    if confmat.shape != (2,2):\n",
    "        tn, fp, fn, tp = 0, 0, 0, confmat[0][0] \n",
    "    else:\n",
    "        tn, fp, fn, tp = confmat.ravel()\n",
    "    \n",
    "    # Calculate performance metrics\n",
    "    # Accuracy\n",
    "    acc = (tp+tn)/(tn + fp + fn + tp)        \n",
    "    \n",
    "    # Sensitivity (True Positive Rate)\n",
    "    if tp == 0 and fn == 0:\n",
    "        sen = 0\n",
    "    else:\n",
    "        sen = tp/(tp+fn)\n",
    "    \n",
    "    # Specificity\n",
    "    if tn == 0 and fp == 0:\n",
    "        esp = 0\n",
    "    else:\n",
    "        esp = tn/(tn+fp)\n",
    "    \n",
    "    # Positive Predictive Value (recall)\n",
    "    if tp == 0 and fp == 0:\n",
    "        ppv = 0\n",
    "    else:\n",
    "        ppv = tp/(tp+fp)\n",
    "    \n",
    "    # F1 Score    \n",
    "    if sen == 0 and ppv == 0:\n",
    "        fsc = 0\n",
    "    else:\n",
    "        fsc = 2*(sen*ppv)/(sen+ppv)\n",
    "    \n",
    "    # Output the calculated metrics\n",
    "    return acc, sen, esp, ppv, fsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca03263d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def print_performance_metrics(confmat_train, *confmat_test):\n",
    "    '''\n",
    "    Print performance metrics based on confusion matrices.\n",
    "\n",
    "    Parameters:\n",
    "    - confmat_train (array-like): Confusion matrix for the training set.\n",
    "    - *confmat_test (array-like, optional): Confusion matrix for the test set. Multiple test sets can be provided.\n",
    "\n",
    "    Outputs:\n",
    "    - None: Metrics are printed to the console.\n",
    "    '''\n",
    "\n",
    "    # Check if there is a test confusion matrix provided\n",
    "    if not confmat_test:\n",
    "        # If no test confusion matrix, calculate and print metrics for the training set only\n",
    "        acc, sen, esp, ppv, fsc = get_metrics(confmat_train)\n",
    "        print('TRAINING SET METRICS')\n",
    "        print('ACC: %2.2f' % (100 * acc))\n",
    "        print('SEN: %2.2f' % (100 * sen))\n",
    "        print('ESP: %2.2f' % (100 * esp))\n",
    "        print('PPV: %2.2f' % (100 * ppv))\n",
    "        print('F1: %2.2f' % (100 * fsc))\n",
    "    else:\n",
    "        # If test confusion matrix(es) provided, calculate and print metrics for both training and test sets\n",
    "        acc_train, sen_train, esp_train, ppv_train, fsc_train = get_metrics(confmat_train)\n",
    "        acc_test, sen_test, esp_test, ppv_test, fsc_test = get_metrics(confmat_test[0])\n",
    "\n",
    "        print('PERFORMANCE METRICS')\n",
    "        print('\\tTRAIN\\tTEST')\n",
    "        print('ACC:\\t%2.2f\\t%2.2f' % (100 * acc_train, 100 * acc_test))\n",
    "        print('SEN:\\t%2.2f\\t%2.2f' % (100 * sen_train, 100 * sen_test))\n",
    "        print('ESP:\\t%2.2f\\t%2.2f' % (100 * esp_train, 100 * esp_test))\n",
    "        print('PPV:\\t%2.2f\\t%2.2f' % (100 * ppv_train, 100 * ppv_test))\n",
    "        print('F1:\\t%2.2f\\t%2.2f' % (100 * fsc_train, 100 * fsc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "304bbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def plot_confusion_matrix(confmat_train, *confmat_test):\n",
    "    '''\n",
    "    Plot confusion matrix.\n",
    "    - A single confusion matrix\n",
    "    - Comparing two confusion matrices, if provided.\n",
    "\n",
    "    Parameters:\n",
    "    - confmat_train (array-like): Confusion matrix for the training set.\n",
    "    - *confmat_test (array-like, optional): Confusion matrix for the test set. Multiple test sets can be provided.\n",
    "\n",
    "    Outputs:\n",
    "    - None: Confusion matrix plot is displayed.\n",
    "    '''\n",
    "    \n",
    "    if not confmat_test:\n",
    "        fig, ax = plt.subplots(figsize=(3, 3))\n",
    "        ax.matshow(confmat_train, cmap=plt.cm.Blues, alpha=0.5)\n",
    "        for i in range(confmat_train.shape[0]):\n",
    "            for j in range(confmat_train.shape[1]):\n",
    "                ax.text(x=j, y=i, s=confmat_train[i, j], va='center', ha='center')\n",
    "\n",
    "        plt.xlabel('predicted label')\n",
    "        plt.ylabel('true label')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:\n",
    "        fig, ax = plt.subplots(1,2,figsize=(6, 6))\n",
    "        ax[0].matshow(confmat_train, cmap=plt.cm.Blues, alpha=0.5)\n",
    "        for i in range(confmat_train.shape[0]):\n",
    "            for j in range(confmat_train.shape[1]):\n",
    "                ax[0].text(x=j, y=i, s=confmat_train[i, j], va='center', ha='center')\n",
    "\n",
    "        ax[1].matshow(confmat_test[0], cmap=plt.cm.Blues, alpha=0.5)\n",
    "        for i in range(confmat_test[0].shape[0]):\n",
    "            for j in range(confmat_test[0].shape[1]):\n",
    "                ax[1].text(x=j, y=i, s=confmat_test[0][i, j], va='center', ha='center')\n",
    "    \n",
    "        ax[0].set_xlabel('predicted label')\n",
    "        ax[0].set_ylabel('true label')\n",
    "        ax[0].set_title('TRAIN')\n",
    "        \n",
    "        ax[1].set_xlabel('predicted label')\n",
    "        ax[1].set_ylabel('true label')\n",
    "        ax[1].set_title('TEST')\n",
    "    \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "380954a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def analyze_train_test_performance(clf, X_train, X_test, y_train, y_test):\n",
    "    '''\n",
    "    Analyze Train and Test Performance for a Classifier.\n",
    "\n",
    "    Parameters:\n",
    "    - clf: Classifier object (already trained).\n",
    "    - X_train (DataFrame): Feature dataset for training.\n",
    "    - X_test (DataFrame): Feature dataset for testing.\n",
    "    - y_train (Series): Target variable for training.\n",
    "    - y_test (Series): Target variable for testing.\n",
    "\n",
    "    Outputs:\n",
    "    - None: Displays performance metrics, confusion matrices, and ROC curve plot.\n",
    "    '''\n",
    "    \n",
    "    # get predictions\n",
    "    y_pred_train = clf.predict(X_train)\n",
    "    y_pred_test  = clf.predict(X_test)\n",
    "    \n",
    "    # get confusion matrices\n",
    "    confmat_train = confusion_matrix(y_train, y_pred_train)\n",
    "    confmat_test  = confusion_matrix(y_test, y_pred_test)\n",
    "    \n",
    "    # Plot confusion matrices and provide metrics\n",
    "    print_performance_metrics(confmat_train, confmat_test)\n",
    "    plot_confusion_matrix(confmat_train, confmat_test)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    y_prob = clf.predict_proba(X_test)[:,1]\n",
    "    plot_roc_curve(y_test,y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "52a5acb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def plot_roc_curve(y,y_prob):\n",
    "    '''\n",
    "    Plot ROC-AUC Curve and target probability.\n",
    "\n",
    "    Parameters:\n",
    "    - y (array-like): True labels.\n",
    "    - y_prob (array-like): Predicted probabilities for the positive class.\n",
    "\n",
    "    Outputs:\n",
    "    - None: Displays ROC-AUC curve and target probability density plots.\n",
    "    '''\n",
    "    \n",
    "    ejex, ejey, _ = roc_curve(y, y_prob)\n",
    "    roc_auc = auc(ejex, ejey)\n",
    "\n",
    "    plt.figure(figsize = (12,4))\n",
    "    \n",
    "    # ROC-AUC CURVE\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(ejex, ejey, color='darkorange',lw=2, label='AUC = %0.2f' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color=(0.6, 0.6, 0.6), linestyle='--')\n",
    "    plt.plot([0, 0, 1],[0, 1, 1],lw=2, linestyle=':',color='black',label='Perfect classifier')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel('FPR (1-ESP)')\n",
    "    plt.ylabel('SEN')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    # PROB DENSITY \n",
    "    idx_0 = (y==0)\n",
    "    idx_1 = (y==1)\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist(y_prob[idx_0],density=1,bins = 20, label='y=0',alpha=0.5)\n",
    "    plt.hist(y_prob[idx_1],density=1,bins = 20, facecolor='red',label='y=1',alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.xlabel('target probability')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64601128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def hyper_parameters_search(clf, X, y, param_grid, scorer='accuracy', cv=5):\n",
    "    '''\n",
    "    Perform hyperparameter search using GridSearchCV.\n",
    "\n",
    "    Parameters:\n",
    "    - clf: Classifier object.\n",
    "    - X (DataFrame): Feature dataset.\n",
    "    - y (Series): Target variable.\n",
    "    - param_grid (dict): Dictionary of hyperparameter values for the grid search.\n",
    "    - scorer (str, optional): Scoring metric for cross-validation. Default is 'accuracy'.\n",
    "    - cv (int, optional): Number of cross-validation folds. Default is 5.\n",
    "\n",
    "    Outputs:\n",
    "    - grid: GridSearchCV object containing the results of the hyperparameter search.\n",
    "    '''\n",
    "        \n",
    "    grid = GridSearchCV(clf, param_grid = param_grid, scoring = scorer, cv = cv)\n",
    "    grid.fit(X, y)\n",
    "\n",
    "    print(\"best mean cross-validation score: {:.3f}\".format(grid.best_score_))\n",
    "    print(\"best parameters: {}\".format(grid.best_params_))\n",
    "    \n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47b6e54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Óscar´s function\n",
    "def plot_cv_scoring(grid, hyper_parameter, scorer='accuracy', plot_errors=False, log=False):\n",
    "    '''\n",
    "    Plot cross-validated scores over a hyperparameter grid.\n",
    "\n",
    "    Parameters:\n",
    "    - grid: GridSearchCV object containing the results of the hyperparameter search.\n",
    "    - hyper_parameter (str): Name of the hyperparameter to plot.\n",
    "    - scorer (str, optional): Scoring metric for cross-validation. Default is 'f1'.\n",
    "    - plot_errors (bool, optional): If True, plot error bars based on the standard deviation of scores. Default is False.\n",
    "    - log (bool, optional): If True, plot hyperparameter values on a logarithmic scale. Default is False.\n",
    "\n",
    "    Outputs:\n",
    "    - None: Displays the plot.\n",
    "    '''\n",
    "    \n",
    "    scores = np.array(grid.cv_results_['mean_test_score'])\n",
    "    std_scores = grid.cv_results_['std_test_score']\n",
    "        \n",
    "    params = grid.param_grid[hyper_parameter]\n",
    "    \n",
    "    if log:\n",
    "        params = np.log10(params)\n",
    "    \n",
    "    if plot_errors:\n",
    "        print(len(params))\n",
    "        print(len(scores))\n",
    "        plt.errorbar(params,scores,yerr=std_scores, fmt='o-',ecolor='g')\n",
    "    else:\n",
    "        plt.plot(params,scores, 'o-')\n",
    "    plt.xlabel(hyper_parameter,fontsize=14)\n",
    "    plt.ylabel(scorer)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "577738e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate(model, x, y, scoring=None):\n",
    "    \"\"\"Calculates the mean of each given metric for each of the given models through cross-validation.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- model or list of models for cross-validation\n",
    "    x -- 2D array with the dataset features\n",
    "    y -- 1D array with the target variable (\"status\") of the dataset\n",
    "    scoring -- string or list containing the metrics to be used\n",
    "    \n",
    "    Returns:\n",
    "    result -- list with the means of each metric for each fold of the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using StratifiedKFold ensures that each fold in which the dataset is divided\n",
    "    strat_k_fold = StratifiedKFold(n_splits=10, shuffle=True, random_state=20)\n",
    "    \n",
    "    # Use the cross_validate module to calculate metrics\n",
    "    cv = cross_validate(model, x, y, cv=strat_k_fold, scoring=scoring)\n",
    "    \n",
    "    # Save the means of each metric in a dictionary\n",
    "    result = {}                                        # Create a dictionary\n",
    "    for score in cv:                                   # Iterate over each metric calculated in cv\n",
    "        result[score] = round(cv[score].mean(), 3)     # Save the result in a dictionary that is then returned\n",
    "    \n",
    "    # Return the result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "471288ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossValidate2(model, x, y, scoring):\n",
    "    \"\"\"Returns a list of model performance scores through cross-validation.\n",
    "    \n",
    "    Arguments:\n",
    "    model -- model or list of models for cross-validation\n",
    "    x -- 2D array with the dataset features\n",
    "    y -- 1D array with the target variable (\"status\") of the dataset\n",
    "    scoring -- string with the metric to be used\n",
    "    \n",
    "    Returns:\n",
    "    scores -- result of the metric for each model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Using RepeatedKFold ensures that each fold in which the dataset is divided\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=20)\n",
    "    \n",
    "    # Using cross_val_score to assess the suitability of the models\n",
    "    scores = cross_val_score(\n",
    "        model,\n",
    "        x,\n",
    "        y,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        error_score=\"raise\",\n",
    "    )\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25036601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(model_dict, phase):\n",
    "    '''\n",
    "    Visualize the performance of different models on training, cross-validation, or testing data.\n",
    "\n",
    "    Parameters:\n",
    "    - model_dict (dict): Dictionary containing model performance metrics.\n",
    "    - phase (str): Specifies the phase to evaluate ('training', 'cross_validation', or 'testing').\n",
    "\n",
    "    Outputs:\n",
    "    - None: Displays bar charts representing the performance of each model.\n",
    "    '''\n",
    "\n",
    "    # Representamos el rendimiento de cada modelo\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    a = 1\n",
    "    colors = ['b', 'r', 'y', 'g', 'k', 'c']\n",
    "\n",
    "    for name in model_dict:                                                      # Iteramos sobre el nombre de los modelos\n",
    "\n",
    "        labels = list(model_dict[name][phase].keys())                   # Guardamos los nombres de cada métrica\n",
    "        values = list(model_dict[name][phase].values())\n",
    "        \n",
    "\n",
    "        # Separar los valores\n",
    "        if phase == 'training':\n",
    "            training_scores = []\n",
    "            test_scores = []\n",
    "            for item in values:\n",
    "                training_scores.append(item[0])\n",
    "                test_scores.append(item[1])            # Guardamos los valores de cada métrica\n",
    "                \n",
    "        elif phase == 'cross_validation':\n",
    "            labels = labels[2:]\n",
    "            test_scores = values[2:]\n",
    "\n",
    "        plt.subplot(2,3,a)                                                       # Iniciamos la representación\n",
    "        plt.bar(labels, test_scores, color=colors)                                   # Hacemos un gráfico de barras para cada métrica\n",
    "\n",
    "        # Anotamos el valor de cada métrica en el gráfico\n",
    "        for i,j in zip([0, 1, 2, 3, 4], test_scores):\n",
    "            #print(name,i,j)\n",
    "            plt.annotate(round(j, 3), xy=(i-0.2,j), fontsize=15)\n",
    "\n",
    "        # Ponemos los títulos\n",
    "        plt.title(\"{} performance:\".format(name))\n",
    "        plt.suptitle(f\"Performances of Various Models.{phase}\", fontsize=16)\n",
    "\n",
    "        plt.ylim(0,1)\n",
    "\n",
    "        a = a + 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5d5557c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features(features_dict, n):\n",
    "    \"\"\"\n",
    "    Get the top N features based on their importance scores.\n",
    "\n",
    "    Parameters:\n",
    "    - features_dict (dict): Dictionary containing feature names as keys and their importance scores as values.\n",
    "    - n (int): Number of top features to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    List of the top N features.\n",
    "    \"\"\"\n",
    "    \n",
    "    sorted_keys = sorted(features_dict, key=features_dict.get, reverse=True)\n",
    "    return sorted_keys[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2dbbade9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importances(importances, feat_names):\n",
    "    '''\n",
    "    Plot feature importances.\n",
    "\n",
    "    Parameters:\n",
    "    - importances (array-like): Feature importances.\n",
    "    - feat_names (array-like): Names of the features.\n",
    "\n",
    "    Outputs:\n",
    "    - None: Displays a bar chart of feature importances.\n",
    "    '''\n",
    "    \n",
    "    df_importances = pd.Series(importances, index=feat_names)\n",
    "    \n",
    "    plt.figure()\n",
    "    df_importances.plot.bar()\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
